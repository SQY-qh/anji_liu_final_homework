Mamba ä¸ MinGRU ç®—æ³•å®éªŒ
1. å®éªŒç›®çš„ä¸èƒŒæ™¯
1.1 å®éªŒç›®çš„
æœ¬å®éªŒæ—¨åœ¨ï¼š
â€¢ æ·±å…¥ç†è§£ MinGRU å’Œ Mamba ä¸¤ç§åºåˆ—æ¨¡å‹çš„å·¥ä½œåŸç†
â€¢ æŒæ¡çº¯ numpy å®ç°æ·±åº¦å­¦ä¹ æ¨¡å‹çš„æ–¹æ³•
â€¢ å­¦ä¹ æ—¶é—´åºåˆ—é¢„æµ‹ä»»åŠ¡çš„æ•°æ®å¤„ç†å’Œæ¨¡å‹è¯„ä¼°
â€¢ å¯¹æ¯”ä¼ ç»Ÿ RNN ä¸æ–°å‹ SSM æ¨¡å‹çš„æ€§èƒ½å·®å¼‚
â€¢ åŸ¹å…»æ·±åº¦å­¦ä¹ æ¨¡å‹çš„è°ƒè¯•å’Œä¼˜åŒ–èƒ½åŠ›
1.2 èƒŒæ™¯çŸ¥è¯†
1.2.1 åºåˆ—æ¨¡å‹
åºåˆ—æ¨¡å‹æ˜¯ä¸€ç±»å¤„ç†æ—¶é—´æˆ–ç©ºé—´åºåˆ—æ•°æ®çš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œå¹¿æ³›åº”ç”¨äºï¼š
â€¢ è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰
â€¢ æ—¶é—´åºåˆ—é¢„æµ‹ï¼ˆè‚¡ç¥¨ã€å¤©æ°”ã€äº¤é€šï¼‰
â€¢ è¯­éŸ³è¯†åˆ«
â€¢ è§†é¢‘åˆ†æ
1.2.2 MinGRU æ¨¡å‹
MinGRUï¼ˆMinimum Gated Recurrent Unitï¼‰æ˜¯ GRU çš„ç®€åŒ–ç‰ˆæœ¬ï¼Œé€šè¿‡ä¸¤ä¸ªé—¨æ§å•å…ƒ
æ§åˆ¶ä¿¡æ¯æµåŠ¨ï¼š
â€¢ æ›´æ–°é—¨ï¼šå†³å®šä¿ç•™å¤šå°‘æ—§ä¿¡æ¯å’Œæ·»åŠ å¤šå°‘æ–°ä¿¡æ¯
â€¢ é‡ç½®é—¨ï¼šå†³å®šå¦‚ä½•ç»“åˆæ–°è¾“å…¥å’Œæ—§éšè—çŠ¶æ€
1.2.3 Mamba æ¨¡å‹
Mamba æ˜¯ 2023 å¹´æå‡ºçš„åŸºäºçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰çš„æ–°å‹åºåˆ—æ¨¡å‹ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹
ç‚¹ï¼š
â€¢ çº¿æ€§æ—¶é—´å¤æ‚åº¦ï¼Œé€‚åˆé•¿åºåˆ—
â€¢ çŠ¶æ€ç©ºé—´æ¨¡å‹è®¾è®¡ï¼Œé«˜æ•ˆæ•æ‰é•¿æœŸä¾èµ–
â€¢ é€‰æ‹©æ€§æ‰«ææœºåˆ¶ï¼ŒåŠ¨æ€è°ƒæ•´ä¿¡æ¯å¤„ç†
2. ç¯å¢ƒæ­å»º
2.1 ç³»ç»Ÿè¦æ±‚
â€¢ Python 3.8+
â€¢ Windows/macOS/Linux
â€¢ è‡³å°‘ 4GB RAM
2.2 ä¾èµ–å®‰è£…
bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆå¯é€‰ä½†æ¨èï¼‰
python -m venv venv
# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
# Windows
venv\Scripts\activate
# macOS/Linux
source venv/bin/activate
# å®‰è£…ä¾èµ–
pip install numpy==1.24.3 pandas==2.0.3 yfinance==0.2.28 scikitï¿¾learn==1.3.0
2.3 é¡¹ç›®ç»“æ„è¯¦è§£
text
labs-rnn/
â”œâ”€â”€ models/ # æ¨¡å‹å®ç°ç›®å½•
â”‚ â”œâ”€â”€ min_gru.py # MinGRU æ¨¡å‹å®ç°
â”‚ â””â”€â”€ mamba.py # Mamba æ¨¡å‹å®ç°
â”œâ”€â”€ utils/ # å·¥å…·å‡½æ•°ç›®å½•
â”‚ â””â”€â”€ data_loader.py # æ•°æ®åŠ è½½ã€é¢„å¤„ç†å’Œæ‰¹æ¬¡åˆ›å»º
â”œâ”€â”€ data/ # æ•°æ®ç›®å½•ï¼ˆè‡ªåŠ¨åˆ›å»ºï¼‰
â”‚ â””â”€â”€ AAPL_stock_data.csv # ä¸‹è½½çš„è‚¡ç¥¨æ•°æ®
â”œâ”€â”€ test_model.py # å•ä¸ªæ¨¡å‹æµ‹è¯•è„šæœ¬
â”œâ”€â”€ benchmark.py # æ¨¡å‹æ€§èƒ½å¯¹æ¯”è„šæœ¬
â”œâ”€â”€ test_mamba_simple.py # Mamba ç®€å•æµ‹è¯•è„šæœ¬
â””â”€â”€ å®éªŒæŒ‡å—.md # æœ¬è¯¦ç»†å®éªŒæŒ‡å—
2.4 å…³é”®æ–‡ä»¶è¯´æ˜
æ–‡ä»¶åç§° ä¸»è¦åŠŸèƒ½ æ ¸å¿ƒç±»/å‡½æ•°
min_gru.py MinGRU æ¨¡å‹å®ç° MinGRU ç±»ï¼š
forward/backward/update
mamba.py Mamba æ¨¡å‹å®ç° Mamba ç±»ï¼š
forward/backward/update
data_loader.py æ•°æ®å¤„ç† DataLoader ç±»ï¼š
load_yahoo_stock/create
_stock_batches
test_model.py æ¨¡å‹æµ‹è¯• train_min_gru()/train_ma
mba()
benchmark.py æ¨¡å‹å¯¹æ¯” train_model()/validate_m
odel()
3. æ•°æ®å¤„ç†è¯¦è§£
3.1 æ•°æ®ä¸‹è½½
é¡¹ç›®ä½¿ç”¨ yfinance åº“è‡ªåŠ¨ä¸‹è½½ AAPL è‚¡ç¥¨æ•°æ®ï¼ŒåŒ…æ‹¬ï¼š
â€¢ æ—¥æœŸèŒƒå›´ï¼š2010-01-01 è‡³ 2023-12-31
â€¢ æ•°æ®å­—æ®µï¼šOpen, High, Low, Close, Adj Close, Volume
3.2 æ•°æ®é¢„å¤„ç†
python
def load_yahoo_stock(self, ticker='AAPL', start_date='2010-01-01',
end_date='2023-12-31'):
# 1. ä¸‹è½½æ•°æ®
df = yf.download(ticker, start=start_date, end=end_date)
# 2. åªä¿ç•™æ”¶ç›˜ä»·
data = df['Close'].values.reshape(-1, 1)
# 3. æ•°æ®å½’ä¸€åŒ–
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(data)
# 4. ä¿å­˜æ•°æ®
data_path = os.path.join(self.data_dir,
f'{ticker}_stock_data.csv')
df.to_csv(data_path)
return {
'data': scaled_data,
'scaler': scaler,
'original_data': data
}
3.3 æ‰¹æ¬¡åˆ›å»º
python
def create_stock_batches(self, data, seq_len, batch_size):
""" åˆ›å»ºè‚¡ç¥¨æ•°æ®æ‰¹æ¬¡
å‚æ•°ï¼š
data: å½’ä¸€åŒ–åçš„è‚¡ç¥¨æ•°æ® (n_samples, 1)
seq_len: åºåˆ—é•¿åº¦ï¼ˆä½¿ç”¨å‰ seq_len ä¸ªå€¼é¢„æµ‹ä¸‹ä¸€ä¸ªå€¼ï¼‰
batch_size: æ‰¹æ¬¡å¤§å°
è¿”å›ï¼š
batches: æ‰¹æ¬¡åˆ—è¡¨ï¼Œæ¯ä¸ªæ‰¹æ¬¡åŒ…å«(x, y)
x: (seq_len, input_size, batch_size)
y: (1, batch_size)
"""
total_len = len(data)
x = []
y = []
# åˆ›å»ºåºåˆ—æ•°æ®
for i in range(total_len - seq_len):
x.append(data[i:i+seq_len]) # å‰ seq_len ä¸ªæ•°æ®
y.append(data[i+seq_len]) # ä¸‹ä¸€ä¸ªæ•°æ®
x = np.array(x)
y = np.array(y)
# åˆ›å»ºæ‰¹æ¬¡
n_batches = len(x) // batch_size
x = x[:n_batches * batch_size]
y = y[:n_batches * batch_size]
# é‡å¡‘ä¸º (batch_size, n_batches, seq_len, 1)
x = x.reshape(batch_size, n_batches, seq_len, 1).transpose(1,
2, 3, 0)
y = y.reshape(batch_size, n_batches, 1).transpose(1, 2, 0)
batches = []
for i in range(n_batches):
batches.append((x[i], y[i]))
return batches
4. æ¨¡å‹å®ç°è¯¦è§£
4.1 MinGRU æ¨¡å‹å®ç°
4.1.1 åˆå§‹åŒ–æ–¹æ³•
python
def __init__(self, input_size, hidden_size, output_size):
self.input_size = input_size
self.hidden_size = hidden_size
self.output_size = output_size
# åˆå§‹åŒ–æƒé‡
self.W_z = np.random.randn(hidden_size, hidden_size +
input_size) * 0.01 # æ›´æ–°é—¨æƒé‡
self.W_r = np.random.randn(hidden_size, hidden_size +
input_size) * 0.01 # é‡ç½®é—¨æƒé‡
self.W_h = np.random.randn(hidden_size, hidden_size +
input_size) * 0.01 # å€™é€‰éšè—çŠ¶æ€æƒé‡
self.W_y = np.random.randn(output_size, hidden_size) * 0.01 #
è¾“å‡ºæƒé‡
# åˆå§‹åŒ–åç½®
self.b_z = np.zeros((hidden_size, 1))
self.b_r = np.zeros((hidden_size, 1))
self.b_h = np.zeros((hidden_size, 1))
self.b_y = np.zeros((output_size, 1))
# ä¿å­˜æ¢¯åº¦
self.reset_grads()
4.1.2 å‰å‘ä¼ æ’­
python
def forward(self, x, h_prev):
""" å‰å‘ä¼ æ’­
å‚æ•°ï¼š
x: å½“å‰æ—¶é—´æ­¥è¾“å…¥ (input_size, batch_size)
h_prev: å‰ä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ (hidden_size, batch_size)
è¿”å›ï¼š
y: å½“å‰æ—¶é—´æ­¥è¾“å‡º (output_size, batch_size)
h: å½“å‰æ—¶é—´æ­¥çš„éšè—çŠ¶æ€ (hidden_size, batch_size)
"""
# æ‹¼æ¥éšè—çŠ¶æ€å’Œè¾“å…¥
combined = np.concatenate([h_prev, x], axis=0) #
(hidden_size+input_size, batch_size)
# æ›´æ–°é—¨ï¼šå†³å®šä¿ç•™å¤šå°‘æ—§éšè—çŠ¶æ€
z = self.sigmoid(np.dot(self.W_z, combined) + self.b_z) #
(hidden_size, batch_size)
# é‡ç½®é—¨ï¼šå†³å®šå¦‚ä½•ç»“åˆæ–°è¾“å…¥å’Œæ—§éšè—çŠ¶æ€
r = self.sigmoid(np.dot(self.W_r, combined) + self.b_r) #
(hidden_size, batch_size)
# å€™é€‰éšè—çŠ¶æ€
combined_r = np.concatenate([r * h_prev, x], axis=0) #
(hidden_size+input_size, batch_size)
h_tilde = self.tanh(np.dot(self.W_h, combined_r) + self.b_h)
# (hidden_size, batch_size)
# æ–°éšè—çŠ¶æ€ï¼šç»“åˆæ—§éšè—çŠ¶æ€å’Œå€™é€‰éšè—çŠ¶æ€
h = (1 - z) * h_prev + z * h_tilde # (hidden_size,
batch_size)
# è¾“å‡º
y = np.dot(self.W_y, h) + self.b_y # (output_size,
batch_size)
return y, h
4.1.3 åå‘ä¼ æ’­
python
def backward(self, dy, dh_next):
""" åå‘ä¼ æ’­
å‚æ•°ï¼š
dy: è¾“å‡ºæ¢¯åº¦ (output_size, batch_size)
dh_next: ä¸‹ä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€æ¢¯åº¦ (hidden_size,
batch_size)
è¿”å›ï¼š
dx: è¾“å…¥æ¢¯åº¦ (input_size, batch_size)
dh_prev: å‰ä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€æ¢¯åº¦ (hidden_size,
batch_size)
"""
# è¾“å‡ºå±‚æ¢¯åº¦
self.dW_y += np.dot(dy, self.h.T)
self.db_y += np.sum(dy, axis=1, keepdims=True)
dh = np.dot(self.W_y.T, dy) + dh_next # éšè—çŠ¶æ€æ¢¯åº¦
# éšè—å±‚æ¢¯åº¦åˆ†è§£
dh_tilde = dh * self.z # å€™é€‰éšè—çŠ¶æ€æ¢¯åº¦
dz = dh * (self.h_prev - self.h_tilde) # æ›´æ–°é—¨æ¢¯åº¦
# æ›´æ–°é—¨æ¢¯åº¦
dz_sigmoid = self.z * (1 - self.z) * dz # sigmoid å¯¼æ•°
self.dW_z += np.dot(dz_sigmoid, self.combined.T)
self.db_z += np.sum(dz_sigmoid, axis=1, keepdims=True)
# å€™é€‰éšè—çŠ¶æ€æ¢¯åº¦
dh_tilde_tanh = (1 - self.h_tilde**2) * dh_tilde # tanh å¯¼æ•°
self.dW_h += np.dot(dh_tilde_tanh, self.combined_r.T)
self.db_h += np.sum(dh_tilde_tanh, axis=1, keepdims=True)
# é‡ç½®é—¨æ¢¯åº¦
dr_combined = np.dot(self.W_h.T, dh_tilde_tanh)
dr = dr_combined[:self.hidden_size] * self.h_prev
dr_sigmoid = self.r * (1 - self.r) * dr # sigmoid å¯¼æ•°
self.dW_r += np.dot(dr_sigmoid, self.combined.T)
self.db_r += np.sum(dr_sigmoid, axis=1, keepdims=True)
# è¾“å…¥æ¢¯åº¦å’Œå‰ä¸€ä¸ªéšè—çŠ¶æ€æ¢¯åº¦
dx_combined = np.dot(self.W_z.T, dz_sigmoid) +
np.dot(self.W_r.T, dr_sigmoid)
dx = dx_combined[self.hidden_size:] # è¾“å…¥æ¢¯åº¦
dh_prev = dx_combined[:self.hidden_size] + dh * (1 - self.z)
# å‰ä¸€ä¸ªéšè—çŠ¶æ€æ¢¯åº¦
return dx, dh_prev
4.2 Mamba æ¨¡å‹å®ç°
4.2.1 åˆå§‹åŒ–æ–¹æ³•
python
def __init__(self, input_size, hidden_size, output_size,
state_size=64, kernel_size=4):
self.input_size = input_size
self.hidden_size = hidden_size
self.output_size = output_size
self.state_size = state_size
self.kernel_size = kernel_size
# åˆå§‹åŒ–æƒé‡ - ä½¿ç”¨æ›´ç¨³å®šçš„ Xavier åˆå§‹åŒ–
self.W_in = np.random.randn(hidden_size, input_size) *
np.sqrt(2.0 / input_size)
# çŠ¶æ€ç©ºé—´å‚æ•°
self.A = np.ones((state_size, 1)) * -1.0 # å›ºå®šä¸ºè´Ÿå®æ•°ï¼Œç¡®ä¿çŠ¶
æ€è¡°å‡
self.B = np.random.randn(hidden_size, state_size) *
np.sqrt(2.0 / hidden_size)
self.C = np.random.randn(hidden_size, state_size) *
np.sqrt(2.0 / state_size)
self.D = np.random.randn(hidden_size, 1) * 0.1
# è¾“å‡ºæŠ•å½±
self.W_out = np.random.randn(output_size, hidden_size) *
np.sqrt(2.0 / hidden_size)
self.b_out = np.zeros((output_size, 1))
# ä¿å­˜æ¢¯åº¦
self.reset_grads()
# ä¿å­˜å†å²æ¢¯åº¦ï¼Œç”¨äºè°ƒè¯•
self.grad_history = []
4.2.2 å‰å‘ä¼ æ’­
python
def forward(self, x):
""" å‰å‘ä¼ æ’­
å‚æ•°ï¼š
x: è¾“å…¥åºåˆ— (seq_len, input_size, batch_size)
è¿”å›ï¼š
output: è¾“å‡ºåºåˆ— (seq_len, output_size, batch_size)
"""
seq_len, input_size, batch_size = x.shape
# 1. è¾“å…¥æŠ•å½±ï¼šå°†è¾“å…¥æ˜ å°„åˆ°éšè—ç©ºé—´
x_proj = np.zeros((seq_len, self.hidden_size, batch_size))
for t in range(seq_len):
x_proj[t] = np.dot(self.W_in, x[t])
# 2. åˆå§‹åŒ–çŠ¶æ€
s = np.zeros((self.state_size, batch_size)) # çŠ¶æ€ç©ºé—´åˆå§‹åŒ–
y = np.zeros((seq_len, self.hidden_size, batch_size)) # éšè—å±‚
è¾“å‡º
# 3. ä¿å­˜ä¸­é—´ç»“æœï¼Œç”¨äºåå‘ä¼ æ’­
self.s_history = []
self.gate_history = []
self.x_proj = x_proj
# 4. åºåˆ—å¤„ç†ï¼šé€ä¸ªæ—¶é—´æ­¥è®¡ç®—
for t in range(seq_len):
# 4.1 é€‰é€šé—¨ï¼šä½¿ç”¨ SiLU æ¿€æ´»å‡½æ•°
gate = self.silu(x_proj[t])
# 4.2 çŠ¶æ€æ›´æ–°ï¼šä½¿ç”¨çŠ¶æ€ç©ºé—´æ¨¡å‹
s = s * np.exp(self.A) + np.dot(self.B.T, gate) # çŠ¶æ€è¡°å‡
+ æ–°è¾“å…¥
# 4.3 éšè—å±‚è¾“å‡ºï¼šç»“åˆçŠ¶æ€å’Œè¾“å…¥
y[t] = gate * (np.dot(self.C, s) + self.D * x_proj[t])
# 4.4 ä¿å­˜ä¸­é—´ç»“æœ
self.s_history.append(s.copy())
self.gate_history.append(gate.copy())
# 5. æœ€ç»ˆè¾“å‡ºæŠ•å½±
output = np.zeros((seq_len, self.output_size, batch_size))
for t in range(seq_len):
output[t] = np.dot(self.W_out, y[t]) + self.b_out
# 6. ä¿å­˜æ‰€æœ‰éœ€è¦çš„ä¸­é—´ç»“æœ
self.y = y
self.output = output
self.x = x
return output
5. è¯¦ç»†å®éªŒæ­¥éª¤
5.1 ç¯å¢ƒéªŒè¯
bash
# éªŒè¯ Python ç‰ˆæœ¬
python --version # åº”æ˜¾ç¤º 3.8+
# éªŒè¯åº“å®‰è£…
python -c "import numpy, pandas, yfinance, sklearn; print('All
libraries installed successfully!')" 5.2 æ•°æ®ä¸‹è½½ä¸é¢„å¤„ç†
bash
# è¿è¡Œæ•°æ®åŠ è½½æµ‹è¯•
python -c "
from utils.data_loader import DataLoader
loader = DataLoader()
data = loader.load_yahoo_stock(ticker='AAPL')
print(f'Data shape: {data["data"].shape}')
print(f'First 5 data points: {data["data"][:5]}')
"5.3 æ¨¡å‹è®­ç»ƒä¸æµ‹è¯•
5.3.1 è¿è¡Œ MinGRU æ¨¡å‹
bash
# æ–¹æ³• 1ï¼šç›´æ¥è¿è¡Œï¼ˆé»˜è®¤ MinGRUï¼‰
python test_model.py
# æ–¹æ³• 2ï¼šä¿®æ”¹å‚æ•°åè¿è¡Œ
# ä½¿ç”¨æ–‡æœ¬ç¼–è¾‘å™¨æ‰“å¼€ test_model.pyï¼Œä¿®æ”¹å‚æ•°
# model_type = 'min_gru'
python test_model.py
5.3.2 è¿è¡Œ Mamba æ¨¡å‹
bash
# ä¿®æ”¹ test_model.py ä¸­çš„ model_type å‚æ•°ä¸º'mamba'
python -c "
# ä½¿ç”¨ Python è„šæœ¬ä¿®æ”¹å‚æ•°
with open('test_model.py', 'r') as f:
content = f.read()
content = content.replace('model_type = \'min_gru\'', 'model_type
= \'mamba\'')
with open('test_model.py', 'w') as f:
f.write(content)
print('Model type changed to mamba')
"
# è¿è¡Œ Mamba æ¨¡å‹
python test_model.py
5.3.3 è¿è¡Œæ¨¡å‹å¯¹æ¯”æµ‹è¯•
bash
# è¿è¡ŒåŸºå‡†æµ‹è¯•è„šæœ¬
python benchmark.py
5.4 å‚æ•°è°ƒæ•´å®éªŒ
5.4.1 è°ƒæ•´å­¦ä¹ ç‡
bash
# ä¿®æ”¹å­¦ä¹ ç‡ä¸º 0.005
python -c "
with open('test_model.py', 'r') as f:
content = f.read()
content = content.replace('learning_rate = 0.01', 'learning_rate =
0.005')
with open('test_model.py', 'w') as f:
f.write(content)
print('Learning rate changed to 0.005')
"
# è¿è¡Œæµ‹è¯•
python test_model.py
5.4.2 è°ƒæ•´éšè—å±‚å¤§å°
bash
# ä¿®æ”¹éšè—å±‚å¤§å°ä¸º 256
python -c "
with open('test_model.py', 'r') as f:
content = f.read()
content = content.replace('hidden_size = 128', 'hidden_size =
256')
with open('test_model.py', 'w') as f:
f.write(content)
print('Hidden size changed to 256')
"
# è¿è¡Œæµ‹è¯•
python test_model.py
6. ç»“æœåˆ†æä¸å¯è§†åŒ–
6.1 æ€§èƒ½æŒ‡æ ‡
æŒ‡æ ‡åç§° è®¡ç®—å…¬å¼ å«ä¹‰
å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ ğ‘€? =
1
? ?
?=1
?? âˆ’ ?à·œ?
2 à· é¢„æµ‹å€¼ä¸çœŸå®å€¼çš„å¹³å‡å¹³
æ–¹å·®
å‡æ–¹æ ¹è¯¯å·®ï¼ˆRMSEï¼‰ ?ğ‘€? = ğ‘€? é¢„æµ‹å€¼ä¸çœŸå®å€¼çš„å¹³å‡å
å·®
å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰ ğ‘€? =
?
1 à·?
?=1
|?? âˆ’ ?à·œ?| é¢„
å¯¹
æµ‹
å
å€¼
å·®
ä¸çœŸå®å€¼çš„å¹³å‡ç»
å†³å®šç³»æ•°ï¼ˆRÂ²ï¼‰
?
2 = 1 âˆ’
âˆ‘ ?? âˆ’ ?à·œ?
2
âˆ‘ ?? âˆ’ ?
2
æ¨¡å‹è§£é‡Šæ–¹å·®çš„æ¯”ä¾‹
6.2 åŸºå‡†æµ‹è¯•ç»“æœåˆ†æ
6.2.1 è®­ç»ƒè¿‡ç¨‹å¯¹æ¯”
Epoch MinGRU è®­ç»ƒ
æŸå¤±
Mamba è®­ç»ƒ
æŸå¤±
MinGRU éªŒè¯
æŸå¤±
Mamba éªŒè¯
æŸå¤±
1 0.019557 0.011717 0.402195 0.007351
2 0.018652 0.000807 0.387327 0.085414
3 0.017697 0.000759 0.359739 0.001000
4 0.015933 0.000736 0.309980 0.079648
5 0.012945 0.000718 0.231889 0.000733
6.2.2 å…³é”®å‘ç°
1. Mamba æ”¶æ•›é€Ÿåº¦æ›´å¿«ï¼š
ï¿® ä»… 1 ä¸ª epoch åï¼ŒMamba è®­ç»ƒæŸå¤±é™è‡³ 0.0117ï¼Œè€Œ MinGRU ä¸º 0.0196
ï¿® ç¬¬ 2 ä¸ª epoch åï¼ŒMamba è®­ç»ƒæŸå¤±å·²é™è‡³ 0.0008ï¼Œè¿œä½äº MinGRU çš„
0.0187
2. Mamba æœ€ç»ˆæ€§èƒ½æ›´ä¼˜ï¼š
ï¿® è®­ç»ƒæŸå¤±ï¼šMamba (0.0007) vs MinGRU (0.0129)ï¼Œé™ä½äº† 94.46%
ï¿® éªŒè¯æŸå¤±ï¼šMamba (0.0007) vs MinGRU (0.2319)ï¼Œé™ä½äº† 99.68%
3. Mamba è®­ç»ƒæ•ˆç‡æ›´é«˜ï¼š
ï¿® æ¯ä¸ª epoch å¹³å‡æ—¶é—´ï¼šMamba (0.60s) vs MinGRU (0.69s)ï¼Œå¿« 13%
ï¿® æ€»è®­ç»ƒæ—¶é—´ï¼šMamba (3.0s) vs MinGRU (3.45s)ï¼ŒèŠ‚çœ 13%
4. Mamba æ³›åŒ–èƒ½åŠ›æ›´å¼ºï¼š
ï¿® éªŒè¯æŸå¤±è¿œä½äºè®­ç»ƒæŸå¤±ï¼Œè¯´æ˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›å¼º
ï¿® èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰è‚¡ç¥¨æ•°æ®çš„å¤æ‚æ¨¡å¼
6.3 ç»“æœå¯è§†åŒ–
æ·»åŠ å¯è§†åŒ–ä»£ç åˆ° benchmark.py æœ«å°¾ï¼š
python
# æ·»åŠ åˆ° benchmark.py æœ«å°¾
import matplotlib.pyplot as plt
# ç»˜åˆ¶è®­ç»ƒæŸå¤±å¯¹æ¯”
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(range(1, n_epochs+1), min_gru_results['train_losses'],
label='MinGRU')
plt.plot(range(1, n_epochs+1), mamba_results['train_losses'],
label='Mamba')
plt.title('è®­ç»ƒæŸå¤±å¯¹æ¯”')
plt.xlabel('Epoch')
plt.ylabel('MSE Loss')
plt.legend()
plt.grid(True)
# ç»˜åˆ¶éªŒè¯æŸå¤±å¯¹æ¯”
plt.subplot(1, 2, 2)
plt.plot(range(1, n_epochs+1), min_gru_results['valid_losses'],
label='MinGRU')
plt.plot(range(1, n_epochs+1), mamba_results['valid_losses'],
label='Mamba')
plt.title('éªŒè¯æŸå¤±å¯¹æ¯”')
plt.xlabel('Epoch')
plt.ylabel('MSE Loss')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.savefig('mamba_vs_mingru.png')
print("\n å¯è§†åŒ–ç»“æœå·²ä¿å­˜åˆ° mamba_vs_mingru.png")
7. ä»£ç è°ƒè¯•ä¸å¸¸è§é”™è¯¯
7.1 å¸¸è§é”™è¯¯ä¸è§£å†³æ–¹æ¡ˆ
é”™è¯¯ç±»å‹ é”™è¯¯ä¿¡æ¯ åŸå›  è§£å†³æ–¹æ¡ˆ
å¯¼å…¥é”™è¯¯ ModuleNotFound
Error: No module
named 'yfinance' ç¼ºå°‘ä¾èµ–åº“ è¿è¡Œ pip
install
yfinance
æ•°æ®ä¸‹è½½é”™è¯¯ RemoteDataError:
Unable to find
data for ticker
ç½‘ç»œè¿æ¥é—®é¢˜ æ£€æŸ¥ç½‘ç»œè¿æ¥ï¼Œæˆ–
ä½¿ç”¨æœ¬åœ°æ•°æ®
å½¢çŠ¶ä¸åŒ¹é… ValueError:
operands could
not be broadcast
together with
shapes
çŸ©é˜µç»´åº¦ä¸åŒ¹é… æ£€æŸ¥æ¨¡å‹ä¸­çš„çŸ©é˜µ
ä¹˜æ³•ç»´åº¦ï¼Œç¡®ä¿è¾“
å…¥è¾“å‡ºå½¢çŠ¶æ­£ç¡®
æ¢¯åº¦çˆ†ç‚¸ RuntimeWarning:
overflow
encountered in
exp
æŒ‡æ•°è¿ç®—æº¢å‡º å‡å°å­¦ä¹ ç‡ï¼Œæ·»åŠ 
æ¢¯åº¦è£å‰ªï¼Œæˆ–ä½¿ç”¨
æ›´ç¨³å®šçš„åˆå§‹åŒ–
æŸå¤±ä¸ä¸‹é™ è®­ç»ƒæŸå¤±ä¿æŒä¸å˜ æ¢¯åº¦è®¡ç®—é”™è¯¯ æ£€æŸ¥åå‘ä¼ æ’­ä»£
ç ï¼Œç¡®ä¿æ‰€æœ‰å‚æ•°
çš„æ¢¯åº¦éƒ½è¢«æ­£ç¡®è®¡
ç®—å’Œæ›´æ–°
å†…å­˜é”™è¯¯ MemoryError:
Unable to allocate
array
æ‰¹æ¬¡å¤§å°æˆ–åºåˆ—é•¿
åº¦è¿‡å¤§
å‡å°æ‰¹æ¬¡å¤§å°æˆ–åº
åˆ—é•¿åº¦
7.2 è°ƒè¯•æŠ€å·§
5. æ‰“å°å½¢çŠ¶ï¼šåœ¨å…³é”®æ­¥éª¤æ·»åŠ å½¢çŠ¶æ‰“å°ï¼Œç¡®ä¿æ•°æ®æµåŠ¨æ­£ç¡®
print(f'x shape: {x.shape}')
print(f'x_proj shape: {x_proj.shape}')
6. æ£€æŸ¥æ¢¯åº¦ï¼šæ‰“å°æ¢¯åº¦èŒƒæ•°ï¼Œç¡®ä¿æ¢¯åº¦ä¸ä¸º 0print(f'W_in gradient norm:
{np.linalg.norm(model.dW_in)}')
7. ç®€åŒ–æ¨¡å‹ï¼šå…ˆå®ç°ç®€åŒ–ç‰ˆæ¨¡å‹ï¼Œé€æ­¥æ·»åŠ å¤æ‚åŠŸèƒ½
8. ä½¿ç”¨å°æ•°æ®é›†ï¼šå…ˆåœ¨å°æ•°æ®é›†ä¸Šæµ‹è¯•ï¼Œç¡®ä¿æ¨¡å‹èƒ½æ­£å¸¸è®­ç»ƒ
9. å¯è§†åŒ–ä¸­é—´ç»“æœï¼šç»˜åˆ¶éšè—çŠ¶æ€ã€æ¢¯åº¦å˜åŒ–ç­‰ä¸­é—´ç»“æœ
10. å•æ­¥è°ƒè¯•ï¼šä½¿ç”¨ Python è°ƒè¯•å™¨æˆ–æ·»åŠ æ–­ç‚¹ï¼Œé€è¡Œæ‰§è¡Œä»£ç 
8. æ‰©å±•å®éªŒå»ºè®®
8.1 æ¨¡å‹æ”¹è¿›
8.1.1 ä¼˜åŒ–ç®—æ³•æ”¹è¿›
å®ç°æ›´å…ˆè¿›çš„ä¼˜åŒ–ç®—æ³•ï¼Œå¦‚ Adamã€RMSProp ç­‰ï¼š
python
class AdamOptimizer:
def __init__(self, lr=0.001, beta1=0.9, beta2=0.999,
epsilon=1e-8):
self.lr = lr
self.beta1 = beta1
self.beta2 = beta2
self.epsilon = epsilon
self.m = None # ä¸€é˜¶çŸ©ä¼°è®¡
self.v = None # äºŒé˜¶çŸ©ä¼°è®¡
self.t = 0 # è¿­ä»£æ¬¡æ•°
def update(self, model):
# åˆå§‹åŒ–çŸ©ä¼°è®¡
if self.m is None:
self.m = {}
self.v = {}
for name, param in self._get_params(model):
self.m[name] = np.zeros_like(param)
self.v[name] = np.zeros_like(param)
self.t += 1
# æ›´æ–°å‚æ•°
for name, param, grad in
self._get_params_and_grads(model):
# æ›´æ–°ä¸€é˜¶çŸ©ä¼°è®¡
self.m[name] = self.beta1 * self.m[name] + (1 -
self.beta1) * grad
# æ›´æ–°äºŒé˜¶çŸ©ä¼°è®¡
self.v[name] = self.beta2 * self.v[name] + (1 -
self.beta2) * (grad ** 2)
# åå·®æ ¡æ­£
m_hat = self.m[name] / (1 - self.beta1 ** self.t)
v_hat = self.v[name] / (1 - self.beta2 ** self.t)
# æ›´æ–°å‚æ•°
param -= self.lr * m_hat / (np.sqrt(v_hat) +
self.epsilon)
def _get_params(self, model):
# è·å–æ¨¡å‹å‚æ•°
params = []
for name in ['W_in', 'B', 'C', 'D', 'W_out']:
if hasattr(model, name):
params.append((name, getattr(model, name)))
return params
def _get_params_and_grads(self, model):
# è·å–æ¨¡å‹å‚æ•°å’Œæ¢¯åº¦
params_and_grads = []
for name in ['W_in', 'B', 'C', 'D', 'W_out']:
if hasattr(model, name) and hasattr(model, 'd' +
name):
param = getattr(model, name)
grad = getattr(model, 'd' + name)
params_and_grads.append((name, param, grad))
return params_and_grads
8.1.2 æ­£åˆ™åŒ–æ”¹è¿›
æ·»åŠ  L2 æ­£åˆ™åŒ–ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆï¼š
python
def update(self, lr, weight_decay=0.0001):
# æ›´æ–°æ‰€æœ‰å‚æ•°ï¼Œå¹¶æ·»åŠ  L2 æ­£åˆ™åŒ–
self.W_in -= lr * (self.dW_in + weight_decay * self.W_in)
self.B -= lr * (self.dB + weight_decay * self.B)
self.C -= lr * (self.dC + weight_decay * self.C)
self.D -= lr * (self.dD + weight_decay * self.D)
self.W_out -= lr * (self.dW_out + weight_decay * self.W_out)
self.b_out -= lr * self.db_out
8.2 æ•°æ®æ‰©å±•
8.2.1 ä½¿ç”¨æ›´å¤šç‰¹å¾
ä¿®æ”¹ data_loader.pyï¼Œæ·»åŠ æ›´å¤šè‚¡ç¥¨ç‰¹å¾ï¼š
python
def load_yahoo_stock(self, ticker='AAPL', start_date='2010-01-01',
end_date='2023-12-31'):
# ä¸‹è½½æ•°æ®
df = yf.download(ticker, start=start_date, end=end_date)
# ä½¿ç”¨å¤šä¸ªç‰¹å¾ï¼šå¼€ç›˜ä»·ã€æœ€é«˜ä»·ã€æœ€ä½ä»·ã€æ”¶ç›˜ä»·ã€æˆäº¤é‡
features = ['Open', 'High', 'Low', 'Close', 'Volume']
data = df[features].values # (n_samples, n_features)
# æ•°æ®å½’ä¸€åŒ–
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(data)
# ä¿å­˜æ•°æ®
data_path = os.path.join(self.data_dir,
f'{ticker}_stock_data.csv')
df.to_csv(data_path)
return {
'data': scaled_data,
'scaler': scaler,
'original_data': data,
'features': features
}
8.2.2 ä½¿ç”¨å…¶ä»–æ•°æ®é›†
å°è¯•å…¶ä»–æ—¶é—´åºåˆ—æ•°æ®é›†ï¼š
python
# å¤©æ°”æ•°æ®é›†
def load_weather_data(self, city='Beijing'):
# å®ç°å¤©æ°”æ•°æ®åŠ è½½
pass
# äº¤é€šæµé‡æ•°æ®é›†
def load_traffic_data(self, city='Beijing'):
# å®ç°äº¤é€šæ•°æ®åŠ è½½
pass
8.3 æ¶æ„æ‰©å±•
8.3.1 æ·±å±‚ Mamba
å®ç°æ·±å±‚ Mamba æ¨¡å‹ï¼š
python
class DeepMamba:
def __init__(self, input_size, hidden_size, output_size,
num_layers=2):
self.num_layers = num_layers
self.layers = [Mamba(input_size if i == 0 else
hidden_size, hidden_size, hidden_size) for i in range(num_layers)]
self.output_layer = Mamba(hidden_size, hidden_size,
output_size)
def forward(self, x):
for layer in self.layers:
x = layer.forward(x)
return self.output_layer.forward(x)
def backward(self, dout):
dout = self.output_layer.backward(dout)
for layer in reversed(self.layers):
dout = layer.backward(dout)
return dout
def update(self, lr):
self.output_layer.update(lr)
for layer in self.layers:
layer.update(lr)
9. å®éªŒæ€»ç»“ä¸æ€è€ƒ
9.1 å®éªŒæ€»ç»“
æœ¬å®éªŒæˆåŠŸå®ç°äº† MinGRU å’Œ Mamba ä¸¤ç§åºåˆ—æ¨¡å‹ï¼Œå¹¶åœ¨ AAPL è‚¡ç¥¨æ•°æ®ä¸Šè¿›è¡Œäº†
å¯¹æ¯”ã€‚ä¸»è¦æˆæœåŒ…æ‹¬ï¼š
11. æ¨¡å‹å®ç°ï¼šä½¿ç”¨çº¯ numpy å®ç°äº†å®Œæ•´çš„å‰å‘ä¼ æ’­ã€åå‘ä¼ æ’­å’Œå‚æ•°æ›´æ–°
12. æ•°æ®å¤„ç†ï¼šå®ç°äº†å®Œæ•´çš„æ•°æ®ä¸‹è½½ã€é¢„å¤„ç†å’Œæ‰¹æ¬¡åˆ›å»ºæµç¨‹
13. æ€§èƒ½å¯¹æ¯”ï¼šéªŒè¯äº† Mamba æ¨¡å‹åœ¨æ”¶æ•›é€Ÿåº¦ã€æœ€ç»ˆæ€§èƒ½å’Œè®­ç»ƒæ•ˆç‡ä¸Šçš„ä¼˜åŠ¿
14. è°ƒè¯•æŠ€å·§ï¼šæŒæ¡äº†æ·±åº¦å­¦ä¹ æ¨¡å‹çš„è°ƒè¯•å’Œä¼˜åŒ–æ–¹æ³•
9.2 å…³é”®æ€è€ƒ
15. ä¸ºä»€ä¹ˆ Mamba æ¯” MinGRU è¡¨ç°æ›´å¥½ï¼Ÿ
ï¿® Mamba åŸºäºçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼Œèƒ½å¤Ÿé«˜æ•ˆæ•æ‰é•¿æœŸä¾èµ–
ï¿® Mamba ä½¿ç”¨é€‰æ‹©æ€§æ‰«ææœºåˆ¶ï¼ŒåŠ¨æ€è°ƒæ•´ä¿¡æ¯å¤„ç†
ï¿® Mamba å…·æœ‰çº¿æ€§æ—¶é—´å¤æ‚åº¦ï¼Œé€‚åˆé•¿åºåˆ—å¤„ç†
16. Mamba çš„å±€é™æ€§
ï¿® å®ç°å¤æ‚åº¦é«˜äºä¼ ç»Ÿ RNN
ï¿® å¯¹è¶…å‚æ•°æ•æ„Ÿ
ï¿® è®¡ç®—èµ„æºéœ€æ±‚è¾ƒé«˜
17. æœªæ¥ç ”ç©¶æ–¹å‘
ï¿® ä¼˜åŒ– Mamba çš„å®ç°æ•ˆç‡
ï¿® æ¢ç´¢ Mamba åœ¨å…¶ä»–ä»»åŠ¡ä¸Šçš„åº”ç”¨
ï¿® ç»“åˆæ³¨æ„åŠ›æœºåˆ¶è¿›ä¸€æ­¥æå‡æ€§èƒ½
ï¿® ç ”ç©¶ Mamba çš„ç†è®ºæ€§è´¨
