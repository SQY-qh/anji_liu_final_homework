# NumPy 实现 CNN 实验

使用纯 NumPy 实现的卷积神经网络（CNN），用于 MNIST 手写数字分类任务。

## 环境要求

- Python 3.8~3.10
- numpy >= 1.19.0
- matplotlib >= 3.3.0
- tensorflow >= 2.0.0（仅用于加载 MNIST 数据集）

## 安装依赖

```bash
pip install numpy>=1.19.0 matplotlib>=3.3.0 tensorflow>=2.0.0
```

## 运行实验

直接运行主文件：

```bash
python numpy_cnn_mnist.py
```

## 代码结构

### 核心组件

1. **Conv2D** - 2D 卷积层
   - 支持自定义输入/输出通道数、卷积核大小、步幅和填充
   - 实现完整的前向传播和反向传播

2. **MaxPool2D** - 最大池化层
   - 支持自定义池化窗口大小和步幅
   - 记录最大值位置用于反向传播

3. **Dense** - 全连接层
   - 支持自定义输入和输出维度
   - 使用 Xavier 初始化

4. **激活函数**
   - ReLU（包含反向传播）
   - Softmax（数值稳定实现）

5. **CrossEntropyLoss** - 交叉熵损失函数
   - 数值稳定的实现

6. **CNN** - 完整的 CNN 模型
   - 架构：Conv2D → ReLU → MaxPool2D → Flatten → Dense → ReLU → Dense → Softmax

### 功能特性

- ✅ 完整的前向传播和反向传播实现
- ✅ 批量训练支持
- ✅ 训练过程可视化
- ✅ 测试结果可视化
- ✅ 支持快速测试模式（使用少量数据）
- ✅ 数值稳定性处理

## 配置选项

在 `main()` 函数中可以调整以下参数：

```python
# 数据加载
use_reduced_data=True,  # 是否使用减少的数据量（快速测试）
train_samples=1000,      # 训练样本数
test_samples=100         # 测试样本数

# 训练参数
epochs=2,               # 训练轮数
batch_size=32,          # 批量大小
learning_rate=0.01      # 学习率
```

## 预期输出

程序运行时会输出：
1. 数据加载信息
2. 每轮训练的训练损失和准确率
3. 每轮测试的测试损失和准确率
4. 总训练时间和每轮平均训练时间
5. 两个可视化图表：
   - 测试样本预测结果（10个样本）
   - 训练曲线（损失和准确率）

## 性能优化建议

1. **使用完整数据集**：将 `use_reduced_data=False` 可以获得更好的性能
2. **增加训练轮数**：将 `epochs` 设置为 20 或更多
3. **调整学习率**：根据训练情况调整 `learning_rate`
4. **调整批量大小**：根据内存情况调整 `batch_size`

## 注意事项

- 纯 NumPy 实现，无 GPU 加速，训练速度较慢
- 完整数据集（60000 训练样本）训练 20 轮约需 1-2 小时
- 建议先用少量数据测试代码正确性，再使用完整数据集

## 扩展实验

代码支持以下扩展：
- 增加卷积层深度
- 添加 Dropout 层
- 实现平均池化层
- 使用不同的优化算法（Momentum、Adam）
- 数据增强
- 批归一化

详细扩展方法请参考实验文档。

