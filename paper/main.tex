\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{fontspec}
\usepackage{xeCJK}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{amsmath}
\graphicspath{{paper/figs/}}

\setmainfont{Times New Roman}
\setCJKmainfont{PingFang SC}
\title{A Brief Comparative Study of MLP, CNN, MinGRU, Mamba, and Transformer}
\author{\textbf{刘安吉}}
\date{}

\begin{document}
\maketitle

\begin{abstract}
We present a comparative study of five model families---MLP, CNN, MinGRU, Mamba, and Transformer---on small-scale image and text datasets: MNIST, CIFAR-10, and SMS Spam (word-level). Within a unified training framework, we perform per-model ablations and cross-model comparisons with checkpoint-resume to avoid rerunning completed sub-experiments. Our findings highlight modality-driven inductive biases: CNNs and Transformers excel at images, while sequence-centric models (MinGRU, Mamba, Transformer) achieve top performance on text. We discuss optimization choices, positional encodings, and capacity scaling, providing practical guidance for model selection on small datasets.
\end{abstract}

\section{Introduction}
Selecting architectures for small datasets requires balancing inductive bias, parameter efficiency, and training stability. We evaluate five model families across images and text within a unified, reproducible codebase. Our contributions are: (i) a consistent training pipeline with automatic resume and ablation orchestration; (ii) robust dataset loaders and preprocessing across modalities; (iii) an empirical analysis of inductive biases and optimization choices; and (iv) clear visualizations with dynamically scaled axes. We ground the study on canonical datasets: MNIST\textsuperscript{\cite{lecun1998}}, CIFAR-10\textsuperscript{\cite{krizhevsky2009}}, and SMS Spam, and we compare modern sequence models (MinGRU, Mamba\textsuperscript{\cite{gu2023}}, Transformer\textsuperscript{\cite{vaswani2017}}) with classic baselines (MLP, CNN).

\section{Related Work}
MLP uses stacked linear layers with nonlinear activations for classification and treats inputs as flattened vectors, lacking explicit spatial or temporal priors. CNN introduces spatial locality via convolutional kernels and pooling, extracting hierarchical features in images; batch normalization and wider channels improve optimization and representation. MinGRU is a minimal gated recurrent unit variant that updates hidden states along time, suitable for sequences with modest temporal dependencies. Mamba\textsuperscript{\cite{gu2023}} is a selective state-space sequence model that learns dynamic states and decays to capture long-range dependencies with linear-time complexity. Transformer\textsuperscript{\cite{vaswani2017}} employs self-attention and positional encodings to model token interactions; patch embeddings adapt it to images, while token embeddings serve text.

\section{Methodology}
\subsection{Architectures}
We implement compact yet representative variants: MLP with two hidden blocks; CNN with two convolutional blocks for images and 1D TextCNN for text; MinGRU as a minimal gated RNN; Mamba with selective state dynamics; and Transformers using patch embedding for images and token embedding for text with sinusoidal positional encodings. Ablations vary widths, depths, kernel sizes, batch normalization, hidden/state sizes, and attention configuration.

\subsection{Implementation Details}
Training is orchestrated by \texttt{classify/train.py}, datasets by \texttt{classify/data.py}, and CLI entry \texttt{classify/experiments.py}. Resume logic detects curve files and merges into \texttt{ablation\_summary.json} to skip completed runs and continue interrupted ones. Sequence tasks enable gradient clipping (norm=1.0). Transformers use AdamW with cosine scheduler; other models use Adam. We standardize normalization across datasets and report per-epoch loss and accuracy. All figures are regenerated from \texttt{ablation\_summary.json}, with dynamic axis scaling and legends included.

\subsection{Preprocessing and Metrics}
MNIST and CIFAR-10 are normalized by dataset statistics. SMS Spam uses lowercased word tokenization, top-\texttt{K} vocabulary, and fixed sequence length with padding. Metrics are classification accuracy and cross-entropy loss computed on train and test splits per epoch.

\section{Experimental Setup}
\subsection{Codebase}
All models share a common training pipeline (\texttt{classify/train.py}) with unified logging, curve saving, and resume logic via \texttt{ablation\_summary.json}. Datasets are loaded from \texttt{classify/data.py}; experiments are orchestrated by \texttt{classify/experiments.py}. Figures are regenerated by \texttt{paper/make\_figs.py}.

\subsection{Datasets}
\textbf{MNIST} (28\,\texttt{x}\,28 grayscale, 10 classes)\textsuperscript{\cite{lecun1998}}.\newline
\textbf{CIFAR-10} (32\,\texttt{x}\,32 RGB, 10 classes)\textsuperscript{\cite{krizhevsky2009}}.\newline
\textbf{SMS Spam} word-level text classification with a top-\texttt{K} vocabulary and sequences of length 50 (2 classes).

\subsection{Models and Ablations}
\textbf{MLP} widths (256,128) with ReLU baseline; wide variant (512,256); Tanh activation variant.\newline
\textbf{CNN} for images uses two conv blocks with optional BN and dropout; ablations include wider channels and larger kernels. For SMS Spam, a 1D TextCNN variant with token embeddings and global pooling.\newline
\textbf{MinGRU} sequence input size aligned to dataset features; hidden sizes vary and time dimension controls scan direction.\newline
\textbf{Mamba} selective state size tuned per dataset; wider hidden layers and larger state provide ablations.\newline
\textbf{Transformer} uses patch embeddings for images and token embeddings for text; ablations vary depth, heads, and embed dimensions while enforcing divisibility constraints.

\subsection{Training Protocol}
We train for 8 epochs on MNIST, 15 on CIFAR-10, and 15 on SMS Spam. Optimizers are Adam for MLP/CNN/MinGRU/Mamba and AdamW with cosine annealing for sequence Transformers. Gradient clipping is enabled on sequence tasks. We report train/test curves for loss and accuracy.

\section{Results}
We provide cross-model comparisons on each dataset and per-model ablations. Curves are line plots without markers; axes are dynamically scaled per figure to avoid clipping. Legends are included for clarity.

\subsection{Overview}
We summarize trends directly in the figures to maintain visual clarity. Cross-model plots provide base-variant comparisons over epochs, while ablation plots show the effect of depth/width/state/patch/token choices. For each dataset, we highlight stability and generalization through the shape of loss and accuracy curves and convergence speed. We reference figures explicitly using labels.

\subsection{MNIST}
\begin{figure}[h]
\centering
\includegraphics[width=0.48\textwidth]{paper/figs/mnist_cross_model_test_acc.pdf}
\includegraphics[width=0.48\textwidth]{paper/figs/mnist_cross_model_test_loss.pdf}
\caption{MNIST cross-model accuracy and loss (base variants).}\label{fig:mnist_cross}
\end{figure}
In Figure \ref{fig:mnist_cross}, CNN and Transformer reach the highest test accuracy and lowest loss with smooth convergence. MLP trails but converges reliably due to MNIST simplicity. MinGRU and Mamba improve steadily yet remain behind image-specialized models, reflecting the advantage of spatial priors. Curves indicate stable training without overfitting; the Transformer benefits from adequate embedding dimension and positional encoding.

\subsection{CIFAR-10}
\begin{figure}[h]
\centering
\includegraphics[width=0.48\textwidth]{paper/figs/cifar10_cross_model_test_acc.pdf}
\includegraphics[width=0.48\textwidth]{paper/figs/cifar10_cross_model_test_loss.pdf}
\caption{CIFAR-10 cross-model accuracy and loss (base variants).}\label{fig:cifar_cross}
\end{figure}
In Figure \ref{fig:cifar_cross}, CNN and Transformer again outperform other families on natural images. Wider CNNs with batch normalization reduce loss and stabilize optimization. The Transformer shows steady accuracy gains across epochs as attention aggregates global context. Sequence-centric models lag due to the lack of spatial inductive bias. Loss curves remain within dynamic bounds and indicate consistent generalization.

\subsection{SMS Spam}
\begin{figure}[h]
\centering
\includegraphics[width=0.48\textwidth]{paper/figs/sms_spam_cross_model_test_acc.pdf}
\includegraphics[width=0.48\textwidth]{paper/figs/sms_spam_cross_model_test_loss.pdf}
\caption{SMS Spam cross-model accuracy and loss (base variants).}\label{fig:sms_cross}
\end{figure}
Figure \ref{fig:sms_cross} shows that sequence-centric models (MinGRU, Mamba, Transformer) dominate text classification. The Transformer with token embeddings delivers the highest accuracy with steadily decreasing loss. TextCNN is competitive but slightly behind attention-based modeling. MLP, lacking sequence priors, underfits relative to sequence models. Gradient clipping contributes to stable curves without spikes.

\subsection{Ablations}
\subsubsection{MNIST}
\begin{figure}[h]
\centering
\includegraphics[width=0.48\textwidth]{paper/figs/mnist_cnn_ablation_test_acc.pdf}
\includegraphics[width=0.48\textwidth]{paper/figs/mnist_cnn_ablation_test_loss.pdf}
\caption{MNIST CNN ablations.}\label{fig:mnist_cnn_ablation}
\end{figure}
As shown in Figure \ref{fig:mnist_cnn_ablation}, wider channels and batch normalization improve accuracy and reduce loss; larger kernels add marginal gains. The best setting balances capacity and regularization without overfitting, evidenced by a near-monotonic loss decrease.

\begin{figure}[h]
\centering
\includegraphics[width=0.48\textwidth]{paper/figs/mnist_mlp_ablation_test_acc.pdf}
\includegraphics[width=0.48\textwidth]{paper/figs/mnist_mlp_ablation_test_loss.pdf}
\caption{MNIST MLP ablations.}\label{fig:mnist_mlp_ablation}
\end{figure}
Figure \ref{fig:mnist_mlp_ablation} indicates that wider MLP improves early epochs but saturates; Tanh reduces overfitting yet limits peak accuracy; the ReLU baseline offers a reasonable trade-off on MNIST.

\begin{figure}[h]
\centering
\includegraphics[width=0.48\textwidth]{paper/figs/mnist_mingru_ablation_test_acc.pdf}
\includegraphics[width=0.48\textwidth]{paper/figs/mnist_mingru_ablation_test_loss.pdf}
\caption{MNIST MinGRU ablations.}\label{fig:mnist_mingru_ablation}
\end{figure}
Figure \ref{fig:mnist_mingru_ablation} shows that larger hidden sizes yield smoother convergence but do not surpass CNN/Transformer; time-dimension scan variants slightly alter learning dynamics without dramatic differences, consistent with MNIST’s spatial nature.

\begin{figure}[h]
\centering
\includegraphics[width=0.48\textwidth]{paper/figs/mnist_mamba_ablation_test_acc.pdf}
\includegraphics[width=0.48\textwidth]{paper/figs/mnist_mamba_ablation_test_loss.pdf}
\caption{MNIST Mamba ablations.}\label{fig:mnist_mamba_ablation}
\end{figure}
In Figure \ref{fig:mnist_mamba_ablation}, increasing state size and width improves stability and accuracy modestly; however, image tasks still favor spatial priors over sequence dynamics, reflected in the gap to CNN/Transformer.

\begin{figure}[h]
\centering
\includegraphics[width=0.48\textwidth]{paper/figs/mnist_transformer_ablation_test_acc.pdf}
\includegraphics[width=0.48\textwidth]{paper/figs/mnist_transformer_ablation_test_loss.pdf}
\caption{MNIST Transformer ablations.}\label{fig:mnist_transformer_ablation}
\end{figure}
Figure \ref{fig:mnist_transformer_ablation} demonstrates that larger embedding dimensions with divisibility-constrained heads improve performance; deeper encoders help until diminishing returns. Positional encoding and patch-size choices maintain stable training and low loss.

\subsubsection{CIFAR-10}
\begin{figure}[h]
\centering
\includegraphics[width=0.48\textwidth]{paper/figs/cifar10_cnn_ablation_test_acc.pdf}
\includegraphics[width=0.48\textwidth]{paper/figs/cifar10_cnn_ablation_test_loss.pdf}
\caption{CIFAR-10 CNN ablations.}\label{fig:cifar_cnn_ablation}
\end{figure}
Figure \ref{fig:cifar_cnn_ablation} shows that wider CNN with BN consistently lowers loss and raises accuracy; larger kernels aid feature capture but BN remains the dominant stabilizer. Curves show robust generalization with no late-epoch degradation.

\begin{figure}[h]
\centering
\includegraphics[width=0.48\textwidth]{paper/figs/cifar10_mlp_ablation_test_acc.pdf}
\includegraphics[width=0.48\textwidth]{paper/figs/cifar10_mlp_ablation_test_loss.pdf}
\caption{CIFAR-10 MLP ablations.}\label{fig:cifar_mlp_ablation}
\end{figure}
In Figure \ref{fig:cifar_mlp_ablation}, MLP variants underperform due to absent spatial priors; wider networks help early learning but plateau below CNN/Transformer, as seen in loss plateaus.

\begin{figure}[h]
\centering
\includegraphics[width=0.48\textwidth]{paper/figs/cifar10_mingru_ablation_test_acc.pdf}
\includegraphics[width=0.48\textwidth]{paper/figs/cifar10_mingru_ablation_test_loss.pdf}
\caption{CIFAR-10 MinGRU ablations.}\label{fig:cifar_mingru_ablation}
\end{figure}
Figure \ref{fig:cifar_mingru_ablation} illustrates that hidden-size increases smooth training yet cannot match methods with spatial or global context. The gap underscores the importance of image-specific priors.

\begin{figure}[h]
\centering
\includegraphics[width=0.48\textwidth]{paper/figs/cifar10_mamba_ablation_test_acc.pdf}
\includegraphics[width=0.48\textwidth]{paper/figs/cifar10_mamba_ablation_test_loss.pdf}
\caption{CIFAR-10 Mamba ablations.}\label{fig:cifar_mamba_ablation}
\end{figure}
Figure \ref{fig:cifar_mamba_ablation} shows that larger state and width improve accuracy gradually; however, the sequence bias remains suboptimal on images compared to CNN/Transformer. Loss curves are stable with modest gains across epochs.

\begin{figure}[h]
\centering
\includegraphics[width=0.48\textwidth]{paper/figs/cifar10_transformer_ablation_test_acc.pdf}
\includegraphics[width=0.48\textwidth]{paper/figs/cifar10_transformer_ablation_test_loss.pdf}
\caption{CIFAR-10 Transformer ablations.}\label{fig:cifar_transformer_ablation}
\end{figure}
Figure \ref{fig:cifar_transformer_ablation} indicates that increasing depth and embedding dimension yields the best improvements, provided head divisibility holds. Cosine scheduling with AdamW prevents overfitting and steadily lowers loss.
\subsubsection{SMS Spam}
\begin{figure}[h]
\centering
\includegraphics[width=0.48\textwidth]{paper/figs/sms_spam_mingru_ablation_test_acc.pdf}
\includegraphics[width=0.48\textwidth]{paper/figs/sms_spam_mingru_ablation_test_loss.pdf}
\caption{SMS Spam MinGRU ablations.}\label{fig:sms_mingru_ablation}
\end{figure}
Figure \ref{fig:sms_mingru_ablation} shows that larger hidden sizes speed convergence and improve peak accuracy. The recurrence efficiently models token sequences; curves show strong generalization.

\begin{figure}[h]
\centering
\includegraphics[width=0.48\textwidth]{paper/figs/sms_spam_mamba_ablation_test_acc.pdf}
\includegraphics[width=0.48\textwidth]{paper/figs/sms_spam_mamba_ablation_test_loss.pdf}
\caption{SMS Spam Mamba ablations.}\label{fig:sms_mamba_ablation}
\end{figure}
Figure \ref{fig:sms_mamba_ablation} shows that bigger state sizes and wider layers provide steady gains; selective state dynamics handle long-range dependencies well, reflected in decreasing loss and rising accuracy.

\begin{figure}[h]
\centering
\includegraphics[width=0.48\textwidth]{paper/figs/sms_spam_transformer_ablation_test_acc.pdf}
\includegraphics[width=0.48\textwidth]{paper/figs/sms_spam_transformer_ablation_test_loss.pdf}
\caption{SMS Spam Transformer ablations.}\label{fig:sms_transformer_ablation}
\end{figure}
Figure \ref{fig:sms_transformer_ablation} shows that token embeddings with sufficient capacity lead to the highest accuracy; deeper encoders maintain low loss. Positional encoding is essential; curves remain smooth without instability.

\begin{figure}[h]
\centering
\includegraphics[width=0.48\textwidth]{paper/figs/sms_spam_cnn_ablation_test_acc.pdf}
\includegraphics[width=0.48\textwidth]{paper/figs/sms_spam_cnn_ablation_test_loss.pdf}
\caption{SMS Spam CNN ablations.}\label{fig:sms_cnn_ablation}
\end{figure}
Figure \ref{fig:sms_cnn_ablation} indicates that TextCNN is competitive; wider channels and moderate dropout help regularization. Nevertheless, attention-based modeling retains an edge on word sequences.



\section{Conclusion}
Across MNIST, CIFAR-10, and SMS Spam, results corroborate the importance of inductive bias: CNNs and Transformers suit images, while sequence models (MinGRU, Mamba, Transformer) suit text. Proper capacity, positional encoding, and optimization are critical. For small datasets, moderate model sizes with stable training and resume logic offer strong performance with efficient experimentation. Limitations include small-scale settings and simple tokenization; future work may explore advanced augmentations, longer training schedules, and larger vocabularies.

\section{References}
\begin{thebibliography}{9}
\bibitem{lecun1998} LeCun, Y., Bottou, L., Bengio, Y., Haffner, P. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 1998.
\bibitem{krizhevsky2009} Krizhevsky, A. Learning multiple layers of features from tiny images. Technical report, 2009.
\bibitem{hochreiter1997} Hochreiter, S., Schmidhuber, J. Long short-term memory. Neural Computation, 1997.
\bibitem{vaswani2017} Vaswani, A. et al. Attention Is All You Need. NeurIPS, 2017.
\bibitem{gu2023} Gu, A. et al. Mamba: Selective State Space Models. 2023.
\end{thebibliography}

\end{document}
